{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "2psAfOQtHIX3",
        "outputId": "ca421c77-a5eb-48f7-eecd-6292bbb59c49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1384882573.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m ]\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"emotion\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0memotions\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# -------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ],
      "source": [
        "# =====================================\n",
        "# Emotion Detection from Text\n",
        "# User-Interactive Version\n",
        "# =====================================\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -------------------------------------\n",
        "# 1. Dataset (Created Manually)\n",
        "# -------------------------------------\n",
        "texts = [\n",
        "    \"I am very happy today\",\"I feel joyful\",\"This is the best day\",\n",
        "    \"I am smiling\",\"I feel excited\",\n",
        "    \"I am sad\",\"I feel lonely\",\"I miss my family\",\n",
        "    \"I feel depressed\",\"I am unhappy\",\n",
        "    \"I am angry\",\"This is frustrating\",\"I am furious\",\n",
        "    \"This makes me mad\",\"I lost my temper\",\n",
        "    \"I am surprised\",\"This shocked me\",\"Unexpected result\",\n",
        "    \"I did not expect this\",\"Big surprise\",\n",
        "    \"I feel calm\",\"Nothing special\",\"Normal day\",\n",
        "    \"Just average\",\"I feel relaxed\",\n",
        "    \"I feel scared\",\"I am nervous\",\"I feel fear\",\n",
        "    \"This is frightening\",\"I am anxious\",\n",
        "    \"I am proud\",\"I achieved my goal\",\"Great achievement\",\n",
        "    \"I feel confident\",\"Success feels good\",\n",
        "    \"I am disappointed\",\"This hurt me\",\"I expected better\",\n",
        "    \"I feel let down\",\"My hopes failed\",\n",
        "    \"I feel stressed\",\"Too much pressure\",\"I am tired\",\n",
        "    \"I feel exhausted\",\"I need rest\",\n",
        "    \"I am grateful\",\"Thankful for support\",\"Feeling blessed\",\n",
        "    \"I appreciate this\"\n",
        "]\n",
        "\n",
        "emotions = [\n",
        "    \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "    \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "    \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "    \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\",\n",
        "    \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\",\n",
        "    \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "    \"proud\",\"proud\",\"proud\",\"proud\",\"proud\",\n",
        "    \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "    \"stress\",\"stress\",\"stress\",\"stress\",\"stress\",\n",
        "    \"grateful\",\"grateful\",\"grateful\",\"grateful\",\"grateful\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({\"text\": texts, \"emotion\": emotions})\n",
        "\n",
        "# -------------------------------------\n",
        "# 2. Text Preprocessing\n",
        "# -------------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Feature Extraction\n",
        "# -------------------------------------\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "y = df[\"emotion\"]\n",
        "\n",
        "# -------------------------------------\n",
        "# 4. Train Model\n",
        "# -------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=1\n",
        ")\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------------\n",
        "# 5. Accuracy\n",
        "# -------------------------------------\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "# -------------------------------------\n",
        "# 6. USER INTERACTION\n",
        "# -------------------------------------\n",
        "print(\"\\n--- Emotion Detection System ---\")\n",
        "print(\"Type a sentence to detect emotion\")\n",
        "print(\"Type 'exit' to stop\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter text: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Thank you for using the system.\")\n",
        "        break\n",
        "\n",
        "    cleaned = clean_text(user_input)\n",
        "    vector = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vector)[0]\n",
        "\n",
        "    print(\"Predicted Emotion:\", prediction)\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "PJD7BjWtHhVf",
        "outputId": "0f961de9-3e4f-437e-b65e-989e0cdfcb3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1320206924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clean_text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1320206924.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-z\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Dataset (50 texts, 50 labels)\n",
        "# -------------------------------\n",
        "texts = [\n",
        "    \"I am very happy today\",\"I feel joyful\",\"This is the best day\",\"I am smiling\",\"I feel excited\",\n",
        "    \"I am sad\",\"I feel lonely\",\"I miss my family\",\"I feel depressed\",\"I am unhappy\",\n",
        "    \"I am angry\",\"This is frustrating\",\"I am furious\",\"This makes me mad\",\"I lost my temper\",\n",
        "    \"I am surprised\",\"This shocked me\",\"Unexpected result\",\"I did not expect this\",\"Big surprise\",\n",
        "    \"I feel calm\",\"Nothing special\",\"Normal day\",\"Just average\",\"I feel relaxed\",\n",
        "    \"I feel scared\",\"I am nervous\",\"I feel fear\",\"This is frightening\",\"I am anxious\",\n",
        "    \"I am proud\",\"I achieved my goal\",\"Great achievement\",\"I feel confident\",\"Success feels good\",\n",
        "    \"I am disappointed\",\"This hurt me\",\"I expected better\",\"I feel let down\",\"My hopes failed\",\n",
        "    \"I feel stressed\",\"Too much pressure\",\"I am tired\",\"I feel exhausted\",\"I need rest\",\n",
        "    \"I am grateful\",\"Thankful for support\",\"Feeling blessed\",\"I appreciate this\",\"Thanks a lot\"\n",
        "]\n",
        "\n",
        "emotions = [\n",
        "    \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "    \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "    \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "    \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\",\n",
        "    \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\",\n",
        "    \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "    \"proud\",\"proud\",\"proud\",\"proud\",\"proud\",\n",
        "    \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "    \"stress\",\"stress\",\"stress\",\"stress\",\"stress\",\n",
        "    \"grateful\",\"grateful\",\"grateful\",\"grateful\",\"grateful\"\n",
        "]\n",
        "\n",
        "# Create DataFrame (NO ERROR NOW)\n",
        "df = pd.DataFrame({\n",
        "    \"text\": texts,\n",
        "    \"emotion\": emotions\n",
        "})\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Text Preprocessing\n",
        "# -------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Feature Extraction\n",
        "# -------------------------------\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "y = df[\"emotion\"]\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Train Model\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Accuracy\n",
        "# -------------------------------\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. USER INTERACTION\n",
        "# -------------------------------\n",
        "print(\"\\n--- Emotion Detection System ---\")\n",
        "print(\"Type a sentence to detect emotion\")\n",
        "print(\"Type 'exit' to stop\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter text: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Thank you for using the system.\")\n",
        "        break\n",
        "\n",
        "    cleaned = clean_text(user_input)\n",
        "    vector = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vector)[0]\n",
        "\n",
        "    print(\"Predicted Emotion:\", prediction)\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "QOyFVBPhHvK3",
        "outputId": "befd0085-3c8b-425a-d982-81f3f78a397e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 10.0 %\n",
            "\n",
            "--- Emotion Detection System ---\n",
            "Type a sentence to detect emotion\n",
            "Type 'exit' to stop\n",
            "\n",
            "Enter text: i am curious\n",
            "Predicted Emotion: sad\n",
            "----------------------------------------\n",
            "Enter text: i am frustrate\n",
            "Predicted Emotion: sad\n",
            "----------------------------------------\n",
            "Enter text: i enjoy\n",
            "Predicted Emotion: sad\n",
            "----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4003555041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter text: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download only stopwords (safe)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset (50 samples)\n",
        "# -------------------------------\n",
        "texts = [\n",
        "    \"I am very happy today\",\"I feel joyful\",\"This is the best day\",\"I am smiling\",\"I feel excited\",\n",
        "    \"I am sad\",\"I feel lonely\",\"I miss my family\",\"I feel depressed\",\"I am unhappy\",\n",
        "    \"I am angry\",\"This is frustrating\",\"I am furious\",\"This makes me mad\",\"I lost my temper\",\n",
        "    \"I am surprised\",\"This shocked me\",\"Unexpected result\",\"I did not expect this\",\"Big surprise\",\n",
        "    \"I feel calm\",\"Nothing special\",\"Normal day\",\"Just average\",\"I feel relaxed\",\n",
        "    \"I feel scared\",\"I am nervous\",\"I feel fear\",\"This is frightening\",\"I am anxious\",\n",
        "    \"I am proud\",\"I achieved my goal\",\"Great achievement\",\"I feel confident\",\"Success feels good\",\n",
        "    \"I am disappointed\",\"This hurt me\",\"I expected better\",\"I feel let down\",\"My hopes failed\",\n",
        "    \"I feel stressed\",\"Too much pressure\",\"I am tired\",\"I feel exhausted\",\"I need rest\",\n",
        "    \"I am grateful\",\"Thankful for support\",\"Feeling blessed\",\"I appreciate this\",\"Thanks a lot\"\n",
        "]\n",
        "\n",
        "emotions = [\n",
        "    \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "    \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "    \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "    \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\",\n",
        "    \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\",\n",
        "    \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "    \"proud\",\"proud\",\"proud\",\"proud\",\"proud\",\n",
        "    \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "    \"stress\",\"stress\",\"stress\",\"stress\",\"stress\",\n",
        "    \"grateful\",\"grateful\",\"grateful\",\"grateful\",\"grateful\"\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({\"text\": texts, \"emotion\": emotions})\n",
        "\n",
        "# -------------------------------\n",
        "# Text Preprocessing (NO punkt)\n",
        "# -------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    words = text.split()   # SAFE tokenizer\n",
        "    words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# -------------------------------\n",
        "# Feature Extraction\n",
        "# -------------------------------\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "y = df[\"emotion\"]\n",
        "\n",
        "# -------------------------------\n",
        "# Train Model\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# Accuracy\n",
        "# -------------------------------\n",
        "accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "# -------------------------------\n",
        "# USER INTERACTION\n",
        "# -------------------------------\n",
        "print(\"\\n--- Emotion Detection System ---\")\n",
        "print(\"Type a sentence to detect emotion\")\n",
        "print(\"Type 'exit' to stop\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter text: \")\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Thank you for using the system.\")\n",
        "        break\n",
        "\n",
        "    cleaned = clean_text(user_input)\n",
        "    vector = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(vector)[0]\n",
        "\n",
        "    print(\"Predicted Emotion:\", prediction)\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2l9ga48Ikj2",
        "outputId": "f625ebe1-86cb-4e77-bc89-bd78ba34ba3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import re\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset (Balanced & Improved)\n",
        "# -------------------------------\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I am happy\",\"I feel joyful\",\"I enjoy this\",\"This is fun\",\"I am excited\",\n",
        "        \"I am sad\",\"I feel lonely\",\"I am unhappy\",\"Feeling low\",\"I feel depressed\",\n",
        "        \"I am angry\",\"This is frustrating\",\"I hate this\",\"I am furious\",\"Very annoyed\",\n",
        "        \"I am scared\",\"I feel fear\",\"I am nervous\",\"This is worrying\",\"I feel anxious\",\n",
        "        \"I am surprised\",\"This shocked me\",\"Unexpected result\",\"Wow amazing\",\"I did not expect this\",\n",
        "        \"I feel normal\",\"Nothing special\",\"Just okay\",\"Average day\",\"I am fine\",\n",
        "        \"I am curious\",\"I want to learn\",\"I am interested\",\"Thinking deeply\",\"I am calm\"\n",
        "    ],\n",
        "    \"emotion\": [\n",
        "        \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "        \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "        \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "        \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "        \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\",\n",
        "        \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\",\n",
        "        \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# -------------------------------\n",
        "# Text Cleaning\n",
        "# -------------------------------\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# -------------------------------\n",
        "# Vectorization\n",
        "# -------------------------------\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "y = df[\"emotion\"]\n",
        "\n",
        "# -------------------------------\n",
        "# Model Training\n",
        "# -------------------------------\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X, y)\n",
        "\n",
        "# -------------------------------\n",
        "# Streamlit UI\n",
        "# -------------------------------\n",
        "st.set_page_config(page_title=\"Emotion Detection\", layout=\"centered\")\n",
        "\n",
        "st.title(\" AI Emotion Detection from Text\")\n",
        "st.write(\"Enter a sentence and detect the emotion using NLP.\")\n",
        "\n",
        "user_input = st.text_area(\"Enter your text here:\")\n",
        "\n",
        "if st.button(\"Detect Emotion\"):\n",
        "    if user_input.strip() == \"\":\n",
        "        st.warning(\"Please enter some text.\")\n",
        "    else:\n",
        "        cleaned = clean_text(user_input)\n",
        "        vector = vectorizer.transform([cleaned])\n",
        "        prediction = model.predict(vector)[0]\n",
        "\n",
        "        emoji_map = {\n",
        "            \"happy\": \"\",\n",
        "            \"sad\": \"\",\n",
        "            \"angry\": \"\",\n",
        "            \"fear\": \"\",\n",
        "            \"surprise\": \"\",\n",
        "            \"neutral\": \"\"\n",
        "        }\n",
        "\n",
        "        st.success(f\"**Detected Emotion:** {prediction.upper()} {emoji_map.get(prediction,'')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LopFXkWOIxi0"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "PJpWhvYvI1Y3",
        "outputId": "74bf1fe2-a84e-4ad0-eae7-f30f2ff0085d"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyngrok'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1171887821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyngrok\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8501\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpublic_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyngrok'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nTcuRreJG0w",
        "outputId": "acf42206-0916-4bf6-beea-1fd8dac4a6cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.2\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-r0jR9iJKAM",
        "outputId": "eb2bed96-7141-4e05-d703-4ea1efe9c2d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import re\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset\n",
        "# -------------------------------\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I am happy\",\"I feel joyful\",\"I enjoy this\",\"This is fun\",\"I am excited\",\n",
        "        \"I am sad\",\"I feel lonely\",\"I am unhappy\",\"Feeling low\",\"I feel depressed\",\n",
        "        \"I am angry\",\"This is frustrating\",\"I hate this\",\"I am furious\",\"Very annoyed\",\n",
        "        \"I am scared\",\"I feel fear\",\"I am nervous\",\"This is worrying\",\"I feel anxious\",\n",
        "        \"I am surprised\",\"This shocked me\",\"Unexpected result\",\"Wow amazing\",\"I did not expect this\",\n",
        "        \"I feel normal\",\"Nothing special\",\"Just okay\",\"Average day\",\"I am fine\",\n",
        "        \"I am curious\",\"I want to learn\",\"I am interested\",\"Thinking deeply\",\"I am calm\"\n",
        "    ],\n",
        "    \"emotion\": [\n",
        "        \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "        \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "        \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "        \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "        \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\",\n",
        "        \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\",\n",
        "        \"neutral\",\"neutral\",\"neutral\",\"neutral\",\"neutral\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing\n",
        "# -------------------------------\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
        "\n",
        "# -------------------------------\n",
        "# Vectorization\n",
        "# -------------------------------\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"clean_text\"])\n",
        "y = df[\"emotion\"]\n",
        "\n",
        "# -------------------------------\n",
        "# Model\n",
        "# -------------------------------\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X, y)\n",
        "\n",
        "# -------------------------------\n",
        "# Streamlit UI\n",
        "# -------------------------------\n",
        "st.set_page_config(page_title=\"Emotion Detection\", layout=\"centered\")\n",
        "\n",
        "st.title(\" AI Emotion Detection from Text\")\n",
        "st.write(\"Enter a sentence to detect emotion\")\n",
        "\n",
        "user_text = st.text_area(\"Enter text:\")\n",
        "\n",
        "if st.button(\"Detect Emotion\"):\n",
        "    if user_text.strip() == \"\":\n",
        "        st.warning(\"Please enter text\")\n",
        "    else:\n",
        "        cleaned = clean_text(user_text)\n",
        "        vector = vectorizer.transform([cleaned])\n",
        "        emotion = model.predict(vector)[0]\n",
        "\n",
        "        emojis = {\n",
        "            \"happy\": \"\",\n",
        "            \"sad\": \"\",\n",
        "            \"angry\": \"\",\n",
        "            \"fear\": \"\",\n",
        "            \"surprise\": \"\",\n",
        "            \"neutral\": \"\"\n",
        "        }\n",
        "\n",
        "        st.success(f\"Detected Emotion: **{emotion.upper()}** {emojis.get(emotion,'')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1iMdLKnmJQFw",
        "outputId": "dad0288e-e26e-4809-fbcb-bf4b97527095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.91.74.72:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/streamlit\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1485, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1406, in main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1873, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1269, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 824, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/cli.py\", line 251, in main_run\n",
            "    _main_run(path_str, args, flag_options=kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/cli.py\", line 308, in _main_run\n",
            "    bootstrap.run(file, is_hello, args, flag_options)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 356, in run\n",
            "    asyncio.run(main())\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 195, in run\n",
            "    return runner.run(main)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\n",
            "    return self._loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 678, in run_until_complete\n",
            "    self.run_forever()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1961, in _run_once\n",
            "    event_list = self._selector.select(timeout)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/selectors.py\", line 468, in select\n",
            "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 42, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 518, in stop\n",
            "    cli_util.print_to_cli(\"  Stopping...\", fg=\"blue\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/cli_util.py\", line 35, in print_to_cli\n",
            "    click.secho(message, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/termui.py\", line 690, in secho\n",
            "    return echo(message, file=file, nl=nl, err=err, color=color)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/utils.py\", line 321, in echo\n",
            "    file.write(out)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 42, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 518, in stop\n",
            "    cli_util.print_to_cli(\"  Stopping...\", fg=\"blue\")\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/cli_util.py\", line 35, in print_to_cli\n",
            "    click.secho(message, **kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/termui.py\", line 690, in secho\n",
            "    return echo(message, file=file, nl=nl, err=err, color=color)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/utils.py\", line 321, in echo\n",
            "    file.write(out)  # type: ignore\n",
            "    ^^^^^^^^^^^^^^^\n",
            "RuntimeError: reentrant call inside <_io.BufferedWriter name='<stdout>'>\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMBlayevKC_L",
        "outputId": "d90042e9-7cc3-446f-be52-7f15a668c59b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit scikit-learn nltk pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kguRugwRKGNZ",
        "outputId": "9eb241c9-794e-46f5-9d60-57a5479e036c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KZ8HC3q8KNjS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I am happy\", \"I feel great\", \"I am excited\", \"I enjoy life\", \"Feeling joyful\",\n",
        "        \"I am sad\", \"I feel lonely\", \"I am depressed\", \"I want to cry\", \"Feeling low\",\n",
        "        \"I am angry\", \"I am furious\", \"This is annoying\", \"I hate this\", \"So much anger\",\n",
        "        \"I am scared\", \"I feel afraid\", \"This is terrifying\", \"I am nervous\", \"I am anxious\",\n",
        "        \"I feel calm\", \"I am relaxed\", \"Feeling peaceful\", \"No stress today\", \"Very calm\",\n",
        "        \"I am curious\", \"I want to learn\", \"Interested in this\", \"I want to explore\", \"Feeling curious\",\n",
        "        \"I am frustrated\", \"This is stressful\", \"I am tired of this\", \"So irritating\", \"Fed up\",\n",
        "        \"I am confident\", \"I believe in myself\", \"Feeling strong\", \"I can do this\", \"Motivated\",\n",
        "        \"I feel loved\", \"I am grateful\", \"Thankful today\", \"Blessed life\", \"Feeling positive\",\n",
        "        \"I am surprised\", \"Unexpected moment\", \"Shocked by news\", \"Didn't expect this\", \"Amazed\"\n",
        "    ],\n",
        "    \"emotion\": [\n",
        "        \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "        \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "        \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "        \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "        \"calm\",\"calm\",\"calm\",\"calm\",\"calm\",\n",
        "        \"curious\",\"curious\",\"curious\",\"curious\",\"curious\",\n",
        "        \"frustrated\",\"frustrated\",\"frustrated\",\"frustrated\",\"frustrated\",\n",
        "        \"confident\",\"confident\",\"confident\",\"confident\",\"confident\",\n",
        "        \"positive\",\"positive\",\"positive\",\"positive\",\"positive\",\n",
        "        \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKAf9KJtKTHU",
        "outputId": "a68c66c6-2763-4b00-d2a0-30d0344612d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 10.0 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"], df[\"emotion\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred)*100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cff4nlfCKVSj",
        "outputId": "8df3b401-bc5d-4eed-8d7f-3743e6d591ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Emotion Detector\", page_icon=\"\", layout=\"centered\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "body {\n",
        "    background-color: #f5f7fa;\n",
        "}\n",
        ".title {\n",
        "    font-size:40px;\n",
        "    color:#4CAF50;\n",
        "    text-align:center;\n",
        "}\n",
        ".box {\n",
        "    background-color:white;\n",
        "    padding:25px;\n",
        "    border-radius:12px;\n",
        "    box-shadow: 0px 4px 10px rgba(0,0,0,0.1);\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.markdown(\"<div class='title'>Emotion Detection System</div>\", unsafe_allow_html=True)\n",
        "st.write(\"\")\n",
        "\n",
        "with st.container():\n",
        "    st.markdown(\"<div class='box'>\", unsafe_allow_html=True)\n",
        "    user_text = st.text_input(\"Enter your sentence:\")\n",
        "\n",
        "    if st.button(\"Detect Emotion\"):\n",
        "        if user_text.strip() == \"\":\n",
        "            st.warning(\"Please enter some text\")\n",
        "        else:\n",
        "            prediction = model.predict([user_text])[0]\n",
        "            st.success(f\"Predicted Emotion: **{prediction.upper()}** \")\n",
        "    st.markdown(\"</div>\", unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzzfcXHMKZbs",
        "outputId": "e17f7a96-c628-4362-ce2b-a968a04315c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streamlit URL: NgrokTunnel: \"https://oversour-sharla-stenothermal.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.set_auth_token(\"36FTDf9F8ZYxjCrZ1NEd7Ocny7B_2ZM6Yv8UqCe8sa1RX4vbf\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Streamlit URL:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vxafGvz7LFB9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I am happy\", \"I feel great\", \"I am excited\", \"I enjoy life\", \"Feeling joyful\",\n",
        "        \"I am sad\", \"I feel lonely\", \"I am depressed\", \"I want to cry\", \"Feeling low\",\n",
        "        \"I am angry\", \"I am furious\", \"This is annoying\", \"I hate this\", \"So much anger\",\n",
        "        \"I am scared\", \"I feel afraid\", \"This is terrifying\", \"I am nervous\", \"I am anxious\",\n",
        "        \"I feel calm\", \"I am relaxed\", \"Feeling peaceful\", \"No stress today\", \"Very calm\",\n",
        "        \"I am curious\", \"I want to learn\", \"Interested in this\", \"I want to explore\", \"Feeling curious\",\n",
        "        \"I am frustrated\", \"This is stressful\", \"I am tired of this\", \"So irritating\", \"Fed up\",\n",
        "        \"I am confident\", \"I believe in myself\", \"Feeling strong\", \"I can do this\", \"Motivated\",\n",
        "        \"I feel loved\", \"I am grateful\", \"Thankful today\", \"Blessed life\", \"Feeling positive\",\n",
        "        \"I am surprised\", \"Unexpected moment\", \"Shocked by news\", \"Didn't expect this\", \"Amazed\"\n",
        "    ],\n",
        "    \"emotion\": [\n",
        "        \"happy\",\"happy\",\"happy\",\"happy\",\"happy\",\n",
        "        \"sad\",\"sad\",\"sad\",\"sad\",\"sad\",\n",
        "        \"angry\",\"angry\",\"angry\",\"angry\",\"angry\",\n",
        "        \"fear\",\"fear\",\"fear\",\"fear\",\"fear\",\n",
        "        \"calm\",\"calm\",\"calm\",\"calm\",\"calm\",\n",
        "        \"curious\",\"curious\",\"curious\",\"curious\",\"curious\",\n",
        "        \"frustrated\",\"frustrated\",\"frustrated\",\"frustrated\",\"frustrated\",\n",
        "        \"confident\",\"confident\",\"confident\",\"confident\",\"confident\",\n",
        "        \"positive\",\"positive\",\"positive\",\"positive\",\"positive\",\n",
        "        \"surprise\",\"surprise\",\"surprise\",\"surprise\",\"surprise\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMNPUOOXLGqf",
        "outputId": "a748d356-5262-48ff-b2c8-d24ef9d40bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 10.0 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"], df[\"emotion\"], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, pred) * 100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "fwo3K1l_LJzU",
        "outputId": "47a69fb9-f189-44f3-9b28-8d621f44df1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ed40d3b9204fd56e86.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ed40d3b9204fd56e86.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def predict_emotion(text):\n",
        "    if text.strip() == \"\":\n",
        "        return \"Please enter some text\"\n",
        "    emotion = model.predict([text])[0]\n",
        "    return f\"Predicted Emotion: {emotion.upper()} \"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=2,\n",
        "        placeholder=\"Type your sentence here...\",\n",
        "        label=\"Enter Text\"\n",
        "    ),\n",
        "    outputs=gr.Textbox(label=\"Emotion Result\"),\n",
        "    title=\" Emotion Detection System\",\n",
        "    description=\"Enter a sentence and detect the emotion using Machine Learning\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daP_KONRMIsF",
        "outputId": "3a2a2451-715a-4717-f556-d09c3c05c3a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 87\n",
            "Total emotions: 29\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    # Positive\n",
        "    (\"I am happy\",\"happy\"),(\"Feeling joyful today\",\"happy\"),(\"Life feels good\",\"happy\"),\n",
        "    (\"I feel excited\",\"excited\"),(\"So thrilled\",\"excited\"),(\"Can't wait\",\"excited\"),\n",
        "    (\"I feel proud\",\"proud\"),(\"Very proud of myself\",\"proud\"),(\"Achievement feels great\",\"proud\"),\n",
        "    (\"I feel confident\",\"confident\"),(\"I believe in myself\",\"confident\"),(\"I can do this\",\"confident\"),\n",
        "    (\"I feel grateful\",\"grateful\"),(\"Thankful today\",\"grateful\"),(\"Feeling blessed\",\"grateful\"),\n",
        "\n",
        "    # Negative\n",
        "    (\"I feel sad\",\"sad\"),(\"Feeling down\",\"sad\"),(\"I want to cry\",\"sad\"),\n",
        "    (\"I am angry\",\"angry\"),(\"This makes me furious\",\"angry\"),(\"So mad right now\",\"angry\"),\n",
        "    (\"I am frustrated\",\"frustrated\"),(\"This is irritating\",\"frustrated\"),(\"Nothing works\",\"frustrated\"),\n",
        "    (\"I feel jealous\",\"jealous\"),(\"I envy them\",\"jealous\"),(\"Feeling insecure\",\"jealous\"),\n",
        "    (\"I feel lonely\",\"lonely\"),(\"No one understands me\",\"lonely\"),(\"I feel isolated\",\"lonely\"),\n",
        "    (\"I feel guilty\",\"guilty\"),(\"I regret this\",\"guilty\"),(\"I feel ashamed\",\"guilty\"),\n",
        "    (\"I feel ashamed\",\"ashamed\"),(\"Feeling embarrassed\",\"ashamed\"),(\"This is humiliating\",\"ashamed\"),\n",
        "    (\"I am scared\",\"fear\"),(\"I feel afraid\",\"fear\"),(\"This is terrifying\",\"fear\"),\n",
        "    (\"I am anxious\",\"anxious\"),(\"Feeling worried\",\"anxious\"),(\"So nervous\",\"anxious\"),\n",
        "    (\"I feel stressed\",\"stressed\"),(\"Too much pressure\",\"stressed\"),(\"Overwhelmed\",\"stressed\"),\n",
        "    (\"I feel depressed\",\"depressed\"),(\"Nothing matters\",\"depressed\"),(\"Feeling empty\",\"depressed\"),\n",
        "\n",
        "    # Neutral / Cognitive\n",
        "    (\"I feel calm\",\"calm\"),(\"Very relaxed\",\"calm\"),(\"Peaceful mind\",\"calm\"),\n",
        "    (\"I am curious\",\"curious\"),(\"I want to learn\",\"curious\"),(\"Interested in this\",\"curious\"),\n",
        "    (\"I am confused\",\"confused\"),(\"I don't understand\",\"confused\"),(\"This is unclear\",\"confused\"),\n",
        "    (\"I feel bored\",\"bored\"),(\"Nothing interesting\",\"bored\"),(\"So dull\",\"bored\"),\n",
        "    (\"I feel surprised\",\"surprised\"),(\"Unexpected news\",\"surprised\"),(\"Did not expect this\",\"surprised\"),\n",
        "    (\"I feel shocked\",\"shocked\"),(\"That was shocking\",\"shocked\"),(\"Unbelievable\",\"shocked\"),\n",
        "\n",
        "    # Social / Complex\n",
        "    (\"I feel loved\",\"loved\"),(\"I am cared for\",\"loved\"),(\"Feeling affection\",\"loved\"),\n",
        "    (\"I feel hopeful\",\"hopeful\"),(\"Things will improve\",\"hopeful\"),(\"I trust the future\",\"hopeful\"),\n",
        "    (\"I feel disappointed\",\"disappointed\"),(\"Expected better\",\"disappointed\"),(\"Let down\",\"disappointed\"),\n",
        "    (\"I feel betrayed\",\"betrayed\"),(\"They broke my trust\",\"betrayed\"),(\"Feeling cheated\",\"betrayed\"),\n",
        "    (\"I feel motivated\",\"motivated\"),(\"Ready to work hard\",\"motivated\"),(\"Feeling driven\",\"motivated\"),\n",
        "    (\"I feel lazy\",\"lazy\"),(\"No energy\",\"lazy\"),(\"Don't want to work\",\"lazy\"),\n",
        "    (\"I feel nostalgic\",\"nostalgic\"),(\"Thinking of old days\",\"nostalgic\"),(\"Missing the past\",\"nostalgic\"),\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"text\",\"emotion\"])\n",
        "print(\"Total samples:\", len(df))\n",
        "print(\"Total emotions:\", df[\"emotion\"].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WaVWMEcMMWEg",
        "outputId": "f8e171fa-bcf9-46ed-ac52-5cf163f98239"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The test_size = 18 should be greater or equal to the number of classes = 29",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-637323833.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emotion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"emotion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \"\"\"\n\u001b[1;32m   1908\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2329\u001b[0m             )\n\u001b[1;32m   2330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_test\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2332\u001b[0m                 \u001b[0;34m\"The test_size = %d should be greater or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m                 \u001b[0;34m\"equal to the number of classes = %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The test_size = 18 should be greater or equal to the number of classes = 29"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"], df[\"emotion\"], test_size=0.2, random_state=42, stratify=df[\"emotion\"]\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2))),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, pred) * 100, \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFjfrf9hM6s1",
        "outputId": "15f19a5e-132a-4868-8086-821795d695a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 90\n",
            "Total emotions: 30\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    # HAPPY\n",
        "    (\"I am happy\",\"happy\"),(\"Feeling joyful\",\"happy\"),(\"Life is good\",\"happy\"),\n",
        "\n",
        "    # SAD\n",
        "    (\"I feel sad\",\"sad\"),(\"Feeling low\",\"sad\"),(\"I want to cry\",\"sad\"),\n",
        "\n",
        "    # ANGRY\n",
        "    (\"I am angry\",\"angry\"),(\"This is annoying\",\"angry\"),(\"Very mad\",\"angry\"),\n",
        "\n",
        "    # FRUSTRATED\n",
        "    (\"I feel frustrated\",\"frustrated\"),(\"This is stressful\",\"frustrated\"),(\"Nothing works\",\"frustrated\"),\n",
        "\n",
        "    # FEAR\n",
        "    (\"I am scared\",\"fear\"),(\"I feel afraid\",\"fear\"),(\"This is terrifying\",\"fear\"),\n",
        "\n",
        "    # ANXIOUS\n",
        "    (\"I feel anxious\",\"anxious\"),(\"Feeling worried\",\"anxious\"),(\"Very nervous\",\"anxious\"),\n",
        "\n",
        "    # JEALOUS\n",
        "    (\"I feel jealous\",\"jealous\"),(\"I envy them\",\"jealous\"),(\"Feeling insecure\",\"jealous\"),\n",
        "\n",
        "    # LONELY\n",
        "    (\"I feel lonely\",\"lonely\"),(\"No one is with me\",\"lonely\"),(\"Feeling isolated\",\"lonely\"),\n",
        "\n",
        "    # CONFIDENT\n",
        "    (\"I feel confident\",\"confident\"),(\"I believe in myself\",\"confident\"),(\"I can do this\",\"confident\"),\n",
        "\n",
        "    # PROUD\n",
        "    (\"I am proud\",\"proud\"),(\"Feeling accomplished\",\"proud\"),(\"Very proud of myself\",\"proud\"),\n",
        "\n",
        "    # GRATEFUL\n",
        "    (\"I feel grateful\",\"grateful\"),(\"Thankful today\",\"grateful\"),(\"Feeling blessed\",\"grateful\"),\n",
        "\n",
        "    # CALM\n",
        "    (\"I feel calm\",\"calm\"),(\"Very relaxed\",\"calm\"),(\"Peaceful mind\",\"calm\"),\n",
        "\n",
        "    # CURIOUS\n",
        "    (\"I am curious\",\"curious\"),(\"I want to learn\",\"curious\"),(\"Interested in this\",\"curious\"),\n",
        "\n",
        "    # BORED\n",
        "    (\"I feel bored\",\"bored\"),(\"Nothing interesting\",\"bored\"),(\"Very dull\",\"bored\"),\n",
        "\n",
        "    # SURPRISED\n",
        "    (\"I am surprised\",\"surprised\"),(\"Unexpected news\",\"surprised\"),(\"Did not expect this\",\"surprised\"),\n",
        "\n",
        "    # SHOCKED\n",
        "    (\"I am shocked\",\"shocked\"),(\"That was shocking\",\"shocked\"),(\"Unbelievable news\",\"shocked\"),\n",
        "\n",
        "    # MOTIVATED\n",
        "    (\"I feel motivated\",\"motivated\"),(\"Ready to work hard\",\"motivated\"),(\"Driven to succeed\",\"motivated\"),\n",
        "\n",
        "    # DEPRESSED\n",
        "    (\"I feel depressed\",\"depressed\"),(\"Nothing matters\",\"depressed\"),(\"Feeling empty\",\"depressed\"),\n",
        "\n",
        "    # STRESSED\n",
        "    (\"I feel stressed\",\"stressed\"),(\"Too much pressure\",\"stressed\"),(\"Overwhelmed\",\"stressed\"),\n",
        "\n",
        "    # CONFUSED\n",
        "    (\"I am confused\",\"confused\"),(\"I don't understand\",\"confused\"),(\"This is unclear\",\"confused\"),\n",
        "\n",
        "    # HOPEFUL\n",
        "    (\"I feel hopeful\",\"hopeful\"),(\"Things will improve\",\"hopeful\"),(\"I trust the future\",\"hopeful\"),\n",
        "\n",
        "    # DISAPPOINTED\n",
        "    (\"I feel disappointed\",\"disappointed\"),(\"Expected better\",\"disappointed\"),(\"Let down\",\"disappointed\"),\n",
        "\n",
        "    # BETRAYED\n",
        "    (\"I feel betrayed\",\"betrayed\"),(\"They broke my trust\",\"betrayed\"),(\"Feeling cheated\",\"betrayed\"),\n",
        "\n",
        "    # LOVED\n",
        "    (\"I feel loved\",\"loved\"),(\"I am cared for\",\"loved\"),(\"Feeling affection\",\"loved\"),\n",
        "\n",
        "    # LAZY\n",
        "    (\"I feel lazy\",\"lazy\"),(\"No energy\",\"lazy\"),(\"Don't want to work\",\"lazy\"),\n",
        "\n",
        "    # NOSTALGIC\n",
        "    (\"I feel nostalgic\",\"nostalgic\"),(\"Missing old days\",\"nostalgic\"),(\"Thinking of the past\",\"nostalgic\"),\n",
        "\n",
        "    # EMBARRASSED\n",
        "    (\"I feel embarrassed\",\"embarrassed\"),(\"This is awkward\",\"embarrassed\"),(\"Feeling ashamed\",\"embarrassed\"),\n",
        "\n",
        "    # GUILTY\n",
        "    (\"I feel guilty\",\"guilty\"),(\"I regret this\",\"guilty\"),(\"Feeling remorse\",\"guilty\"),\n",
        "\n",
        "    # POSITIVE\n",
        "    (\"Feeling positive\",\"positive\"),(\"Good vibes\",\"positive\"),(\"Optimistic mood\",\"positive\"),\n",
        "\n",
        "    # NEGATIVE\n",
        "    (\"Feeling negative\",\"negative\"),(\"Bad mood\",\"negative\"),(\"Everything feels wrong\",\"negative\")\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"text\",\"emotion\"])\n",
        "print(\"Total samples:\", len(df))\n",
        "print(\"Total emotions:\", df[\"emotion\"].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJJvhbPWNAGE",
        "outputId": "2c50e4d7-beb7-4510-e74b-3ef7048ee617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.0 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"text\"],\n",
        "    df[\"emotion\"],\n",
        "    test_size=0.25,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2))),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000))\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", round(accuracy_score(y_test, pred) * 100, 2), \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "8nznbLt5NHIp",
        "outputId": "56c11d74-c3e1-4a0f-b9d9-380ee27ef248"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1836561489.py:9: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Default()) as demo:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d914ab7bf255c731da.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d914ab7bf255c731da.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def detect_emotion(text):\n",
        "    if text.strip() == \"\":\n",
        "        return \"Please enter text.\"\n",
        "    emotion = model.predict([text])[0]\n",
        "    return emotion.upper()\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Default()) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <h1 style='text-align:center;'>Emotion Detection from Text</h1>\n",
        "        <p style='text-align:center; font-size:16px;'>\n",
        "        A Machine Learning system that identifies human emotions from text input\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    input_text = gr.Textbox(\n",
        "        label=\"Enter your text\",\n",
        "        placeholder=\"Type your feelings here...\",\n",
        "        lines=4\n",
        "    )\n",
        "\n",
        "    output_text = gr.Textbox(\n",
        "        label=\"Detected Emotion\"\n",
        "    )\n",
        "\n",
        "    detect_button = gr.Button(\"Detect Emotion\")\n",
        "\n",
        "    detect_button.click(\n",
        "        fn=detect_emotion,\n",
        "        inputs=input_text,\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4QhsQEBtS4N0"
      },
      "outputs": [],
      "source": [
        "!git config --global user.name \"GayathriV12-bit\"\n",
        "!git config --global user.email \"gayathriv237@gmail.com\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sixw9AB4TM2x",
        "outputId": "3f1e07e6-0777-4bd0-816a-85e58de7155c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'emotion-detection' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GayathriV12-bit/emotion-detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "advsL-9QTnp_",
        "outputId": "e818ff53-a7f7-4e3a-a64a-6024e08a9418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "gradio\n",
        "transformers\n",
        "torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzNnPGITTptW",
        "outputId": "d3c90b7c-b6dd-4b1f-ef74-fd68c992e3f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat 'emotion_detection.ipynb': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv emotion_detection.ipynb emotion-detection\n",
        "!mv requirements.txt emotion-detection\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
